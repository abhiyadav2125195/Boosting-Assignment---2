{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00847df3-26a1-4444-849c-6874646c57ce",
   "metadata": {},
   "source": [
    "# Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73275cd9-b0a6-4228-9ecf-c10608377c26",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression, often referred to as just Gradient Boosting or Gradient Boosted Regression Trees (GBRT), is a machine learning technique for regression problems. It is an ensemble learning method that combines the predictions of multiple weak learners (typically decision trees) to create a strong regression model. Gradient Boosting builds the model in a sequential manner, where each new model corrects the errors made by the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e5757-2176-421f-b81c-22f132a39753",
   "metadata": {},
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7787a0a0-5801-4e72-91c6-53682bf984ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5c06b67-1339-4888-bf29-c5eb4388ce11",
   "metadata": {},
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9665c-b239-4f40-bf86-2c34f7a50897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "regressor = GradientBoostingRegressor()\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "grid = GradientBoostingRegressor(regressor,param_grid,cv=5,scoring=\"neg_mean_squared_error\")\n",
    " grid.best_paramas_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dab2b2-652c-45ce-9e28-1b89a387d691",
   "metadata": {},
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db1d00-0dd2-4f89-8c4f-96827f2a20f0",
   "metadata": {},
   "source": [
    "A weak learner in the context of Gradient Boosting is a simple or relatively low-complexity model that performs slightly better than random guessing on a given task. Weak learners are often used as base models in the ensemble. In the case of Gradient Boosting Regression Trees (GBRT), decision trees with limited depth are commonly used as weak learners. These decision trees are often referred to as \"stumps\" when they have very shallow depths.\n",
    "\n",
    "The key characteristic of a weak learner is that it should have low predictive power by itself but can contribute to the ensemble's performance when combined with other weak learners in a boosting algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e510df13-dba7-4d5b-9315-c51233ba182d",
   "metadata": {},
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0dc4fd-7c88-49ab-ac38-a1f780da1848",
   "metadata": {},
   "source": [
    "Start with an initial model (often a simple one, like the mean of target values).\n",
    "Identify the errors made by the initial model.\n",
    "Fit a new model (weak learner) to the errors to correct them.\n",
    "Combine the predictions of the new model with the predictions of the previous model.\n",
    "Repeat steps 2-4 iteratively, with each new model focusing on the remaining errors.\n",
    "The final ensemble of models (strong learner) combines their predictions, resulting in improved accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ae510a-0d16-4872-aa3c-83dc6e5561e9",
   "metadata": {},
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe42a867-cca9-497b-aca1-b0961e0f9ed7",
   "metadata": {},
   "source": [
    "Initialize the ensemble with an initial model (often a simple one, like the mean of target values).\n",
    "Calculate the residuals, which represent the errors made by the current ensemble on the training data.\n",
    "Train a new weak learner (e.g., decision tree) to predict these residuals. The new model focuses on correcting the errors.\n",
    "Combine the predictions of the new model with the predictions of the current ensemble. This combines the strengths of both models.\n",
    "Repeat steps 2-4 for a specified number of iterations (or until convergence).\n",
    "The final ensemble is the sum of all individual models, which collectively form a strong learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec618f53-038d-4d35-9605-7a150e01bee3",
   "metadata": {},
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b1fb47-e679-46dd-bf36-c3db3b2addd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
